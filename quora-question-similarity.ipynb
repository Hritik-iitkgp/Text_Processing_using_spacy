{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":7523176,"sourceType":"datasetVersion","datasetId":4382399}],"dockerImageVersionId":30646,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"#import the neccessary libraries which are required\nimport re\nimport spacy\nimport pandas as pd\nimport numpy as np\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom nltk.stem import PorterStemmer\nfrom collections import Counter\n\n# dataset\nquaries = pd.read_csv('/kaggle/input/quora-doc/queries.csv')\ndocument = pd.read_csv('/kaggle/input/quora-doc/docs.csv')\nquery_eval = pd.read_csv('/kaggle/input/quora-doc/qdrel.csv')","metadata":{"execution":{"iopub.status.busy":"2024-02-04T15:03:16.403484Z","iopub.execute_input":"2024-02-04T15:03:16.403961Z","iopub.status.idle":"2024-02-04T15:03:16.437520Z","shell.execute_reply.started":"2024-02-04T15:03:16.403928Z","shell.execute_reply":"2024-02-04T15:03:16.436203Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"# Task 1 ","metadata":{}},{"cell_type":"markdown","source":"*  Preprocess the docs and queries â€“ remove characters other than alphanumeric or whitespaces.\n*  Correct spelling in the queries and documents using SpaCy. Only for each query with some correction, print the original and corrected query in separate lines, followed by two newlines (\\n).\n*  Tokenize the words in the documents using spacy. Remove all words that occur in less than 5 documents or more than 85% of the documents.\n*  For each query, find the cosine similarity of its vector with that of the documents. Use this to find the top 5 and top 10 most similar documents.\n*  Calculate the Precision@k scores: report P@1, P@5 and P@10 averaged over all queries","metadata":{}},{"cell_type":"code","source":"#Task 1.1 Preprocess the Docs and Queries\ndef clean(text):\n    return re.sub(r'[^a-zA-Z0-9\\s]', '', text)\nquaries['query_text'] = quaries['query_text'].apply(clean)\ndocument['doc_text'] = document['doc_text'].apply(clean)\nprint(quaries['query_text'].head())\n# print(docs_df)","metadata":{"execution":{"iopub.status.busy":"2024-02-04T15:03:21.481117Z","iopub.execute_input":"2024-02-04T15:03:21.481637Z","iopub.status.idle":"2024-02-04T15:03:21.532680Z","shell.execute_reply.started":"2024-02-04T15:03:21.481571Z","shell.execute_reply":"2024-02-04T15:03:21.531531Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"0                   How can ask questions using photos\n1    What is Atal Pension Yojana What are its benefits\n2          Where is starch digested How is it digested\n3          What is a conjecture What are some examples\n4    What can India do to support the people suffer...\nName: query_text, dtype: object\n","output_type":"stream"}]},{"cell_type":"code","source":"#Task1.2 using spacy correct the spelling if there is any misspell\n# print original and corrected query\n# define the spacy in english\ntoken = spacy.load(\"en_core_web_sm\")\ndef correct(text):\n    doc = token(text)\n    corrected =\" \".join(token.text for token in doc)    \n    if(doc.text!=corrected):\n        print(\"In Dataset\",text)\n        print(\"After spell correction\",text)\n    return corrected\nquaries['query_text'] = quaries['query_text'].apply(correct)","metadata":{"execution":{"iopub.status.busy":"2024-02-04T16:12:27.950961Z","iopub.execute_input":"2024-02-04T16:12:27.951551Z","iopub.status.idle":"2024-02-04T16:12:30.161149Z","shell.execute_reply.started":"2024-02-04T16:12:27.951508Z","shell.execute_reply":"2024-02-04T16:12:30.159947Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"#1.3\nall_tokens = []\n# tokenize all doc using \nfor row in document['doc_text']:\n    for t in token(row):\n        all_tokens.append(t.text.lower())    \n# Count frequency of word and remove from each document having < 5\n#  or more than 85% of the documents\nword_freq = Counter(all_tokens)\nmin_freq,doc_freq = 5,0.85\nfiltered_tokens=[]\nfor token, count in word_freq.items():\n    if min_freq <= count <= len(document) * doc_freq:\n        filtered_tokens.append(token)","metadata":{"execution":{"iopub.status.busy":"2024-02-04T15:03:24.696511Z","iopub.execute_input":"2024-02-04T15:03:24.696957Z","iopub.status.idle":"2024-02-04T15:04:48.476967Z","shell.execute_reply.started":"2024-02-04T15:03:24.696911Z","shell.execute_reply":"2024-02-04T15:04:48.475926Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"# Tokenize document and quaries \ntoken = spacy.load(\"en_core_web_sm\")\ntokenized_docs = []\nfor row in document['doc_text']:\n    tokenized_docs.append([tok.text.lower() for tok in token(row) if tok.text.lower() in filtered_tokens])\ntokenized_queries = []\nfor query_text in quaries['query_text']:\n    tokenized_queries.append([tok.text.lower() for tok in token(query_text) if tok.text.lower() in filtered_tokens])\n    \n# for each query and document create tf-idf vector\nvectorizer = TfidfVectorizer()\nmatrix = vectorizer.fit_transform([\" \".join(tokens) for tokens in tokenized_docs + tokenized_queries])\ndoc_matrix = matrix[:len(document)]\nqueries_matrix = matrix[len(document):]","metadata":{"execution":{"iopub.status.busy":"2024-02-04T15:04:48.478788Z","iopub.execute_input":"2024-02-04T15:04:48.479203Z","iopub.status.idle":"2024-02-04T15:06:29.333113Z","shell.execute_reply.started":"2024-02-04T15:04:48.479175Z","shell.execute_reply":"2024-02-04T15:06:29.331605Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"# 1.4 find the cosine similarity of each query\ncosine_similarities = cosine_similarity(queries_matrix, doc_matrix)\n# for each query find the top 5 and top 10 queries from document\ni=0\ntop5=[]\ntop10=[]\nfor similarities in cosine_similarities:\n    top5.append([document.loc[i]['doc_id'] for i in np.argsort(similarities)[::-1][:5]])\n    top10.append([document.loc[i]['doc_id'] for i in np.argsort(similarities)[::-1][:10]])\n    print(f\"\\nTop 5 documents for Query {quaries.loc[i]['query_id']}: {top5[-1]}\")\n    print(f\"Top 10 documents for Query {quaries.loc[i]['query_id']}: {top10[-1]}\")\n    i+=1  ","metadata":{"execution":{"iopub.status.busy":"2024-02-04T15:06:29.334829Z","iopub.execute_input":"2024-02-04T15:06:29.335285Z","iopub.status.idle":"2024-02-04T15:06:29.508811Z","shell.execute_reply.started":"2024-02-04T15:06:29.335219Z","shell.execute_reply":"2024-02-04T15:06:29.507364Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"\nTop 5 documents for Query 4584: [1377, 45, 4412, 2603, 1782]\nTop 10 documents for Query 4584: [1377, 45, 4412, 2603, 1782, 4583, 9179, 2366, 4411, 2602]\n\nTop 5 documents for Query 6588: [7908, 428, 5065, 4108, 1916]\nTop 10 documents for Query 6588: [7908, 428, 5065, 4108, 1916, 2212, 2211, 2328, 2329, 7129]\n\nTop 5 documents for Query 10113: [10319, 3433, 3440, 3439, 3438]\nTop 10 documents for Query 10113: [10319, 3433, 3440, 3439, 3438, 3437, 3436, 3435, 3434, 3432]\n\nTop 5 documents for Query 7957: [10274, 2213, 10298, 2949, 7956]\nTop 10 documents for Query 7957: [10274, 2213, 10298, 2949, 7956, 7582, 10299, 2214, 6327, 6326]\n\nTop 5 documents for Query 5498: [9517, 252, 8399, 3392, 575]\nTop 10 documents for Query 5498: [9517, 252, 8399, 3392, 575, 574, 5456, 6891, 8938, 272]\n\nTop 5 documents for Query 7614: [7613, 5107, 5106, 140, 2178]\nTop 10 documents for Query 7614: [7613, 5107, 5106, 140, 2178, 9758, 280, 3187, 6040, 4222]\n\nTop 5 documents for Query 7301: [7302, 1086, 8576, 1909, 528]\nTop 10 documents for Query 7301: [7302, 1086, 8576, 1909, 528, 7430, 8575, 9761, 5638, 712]\n\nTop 5 documents for Query 4974: [4973, 8788, 8920, 8946, 2483]\nTop 10 documents for Query 4974: [4973, 8788, 8920, 8946, 2483, 3216, 8789, 6147, 6148, 9188]\n\nTop 5 documents for Query 2615: [2614, 6414, 9549, 6014, 3690]\nTop 10 documents for Query 2615: [2614, 6414, 9549, 6014, 3690, 6640, 3302, 1094, 8642, 9550]\n\nTop 5 documents for Query 8950: [8949, 8407, 2182, 1015, 4422]\nTop 10 documents for Query 8950: [8949, 8407, 2182, 1015, 4422, 7999, 7855, 9826, 1638, 5149]\n\nTop 5 documents for Query 9658: [10319, 3433, 3440, 3439, 3438]\nTop 10 documents for Query 9658: [10319, 3433, 3440, 3439, 3438, 3437, 3436, 3435, 3434, 3432]\n\nTop 5 documents for Query 2429: [2428, 4643, 8515, 7504, 1410]\nTop 10 documents for Query 2429: [2428, 4643, 8515, 7504, 1410, 1411, 1280, 1281, 8365, 6520]\n\nTop 5 documents for Query 1803: [6217, 9816, 2697, 1804, 24]\nTop 10 documents for Query 1803: [6217, 9816, 2697, 1804, 24, 9518, 1685, 9817, 6218, 5859]\n\nTop 5 documents for Query 318: [10140, 7490, 317, 7748, 1313]\nTop 10 documents for Query 318: [10140, 7490, 317, 7748, 1313, 830, 9112, 1314, 9124, 9408]\n\nTop 5 documents for Query 6055: [6056, 3494, 9694, 9693, 2507]\nTop 10 documents for Query 6055: [6056, 3494, 9694, 9693, 2507, 2506, 3284, 9802, 5993, 3283]\n\nTop 5 documents for Query 7841: [7840, 3213, 6052, 5376, 1701]\nTop 10 documents for Query 7841: [7840, 3213, 6052, 5376, 1701, 10109, 5375, 9359, 8916, 7095]\n\nTop 5 documents for Query 9733: [9732, 7107, 7108, 5255, 3482]\nTop 10 documents for Query 9733: [9732, 7107, 7108, 5255, 3482, 2495, 9471, 3411, 2827, 3412]\n\nTop 5 documents for Query 4874: [4873, 1115, 1114, 3846, 1745]\nTop 10 documents for Query 4874: [4873, 1115, 1114, 3846, 1745, 10319, 3436, 3441, 3440, 3439]\n\nTop 5 documents for Query 2824: [8710, 2825, 6434, 7469, 7470]\nTop 10 documents for Query 2824: [8710, 2825, 6434, 7469, 7470, 3141, 7932, 7225, 7226, 7152]\n\nTop 5 documents for Query 744: [5775, 8136, 6115, 5776, 2060]\nTop 10 documents for Query 744: [5775, 8136, 6115, 5776, 2060, 2061, 7831, 6222, 6776, 7095]\n\nTop 5 documents for Query 9889: [9890, 7428, 4981, 7047, 1567]\nTop 10 documents for Query 9889: [9890, 7428, 4981, 7047, 1567, 4441, 4144, 4437, 4436, 6211]\n\nTop 5 documents for Query 7822: [7823, 2990, 1784, 2854, 8566]\nTop 10 documents for Query 7822: [7823, 2990, 1784, 2854, 8566, 3587, 5335, 8399, 5747, 5590]\n\nTop 5 documents for Query 10024: [3875, 5028, 6953, 6952, 117]\nTop 10 documents for Query 10024: [3875, 5028, 6953, 6952, 117, 9443, 118, 6894, 10023, 3393]\n\nTop 5 documents for Query 4105: [5518, 7989, 8030, 2802, 1297]\nTop 10 documents for Query 4105: [5518, 7989, 8030, 2802, 1297, 1298, 6865, 6866, 8916, 7165]\n\nTop 5 documents for Query 2791: [2790, 8135, 8134, 8996, 7632]\nTop 10 documents for Query 2791: [2790, 8135, 8134, 8996, 7632, 7631, 9389, 8174, 5467, 5173]\n\nTop 5 documents for Query 7988: [5518, 7989, 8030, 2802, 1297]\nTop 10 documents for Query 7988: [5518, 7989, 8030, 2802, 1297, 1298, 6865, 6866, 8916, 7165]\n\nTop 5 documents for Query 378: [377, 2336, 997, 2098, 6060]\nTop 10 documents for Query 378: [377, 2336, 997, 2098, 6060, 6026, 7101, 8451, 1743, 4223]\n\nTop 5 documents for Query 8808: [8809, 5550, 6672, 8624, 5549]\nTop 10 documents for Query 8808: [8809, 5550, 6672, 8624, 5549, 2175, 2176, 6671, 6307, 859]\n\nTop 5 documents for Query 11167: [7311, 877, 2892, 4018, 2675]\nTop 10 documents for Query 11167: [7311, 877, 2892, 4018, 2675, 6275, 2509, 6312, 4367, 2676]\n\nTop 5 documents for Query 2757: [2756, 3811, 3812, 8956, 7570]\nTop 10 documents for Query 2757: [2756, 3811, 3812, 8956, 7570, 2277, 9704, 8206, 5215, 489]\n\nTop 5 documents for Query 7319: [636, 402, 637, 401, 74]\nTop 10 documents for Query 7319: [636, 402, 637, 401, 74, 3436, 3443, 3442, 3441, 3440]\n\nTop 5 documents for Query 9294: [8362, 5634, 4438, 4439, 7547]\nTop 10 documents for Query 9294: [8362, 5634, 4438, 4439, 7547, 7448, 6168, 5635, 6482, 392]\n\nTop 5 documents for Query 5799: [5800, 2071, 360, 1788, 8678]\nTop 10 documents for Query 5799: [5800, 2071, 360, 1788, 8678, 8663, 5217, 2443, 2442, 3387]\n\nTop 5 documents for Query 5647: [10319, 3433, 3440, 3439, 3438]\nTop 10 documents for Query 5647: [10319, 3433, 3440, 3439, 3438, 3437, 3436, 3435, 3434, 3432]\n\nTop 5 documents for Query 8810: [8811, 1485, 2487, 1486, 4591]\nTop 10 documents for Query 8810: [8811, 1485, 2487, 1486, 4591, 10232, 10233, 4592, 6019, 6020]\n\nTop 5 documents for Query 6483: [8362, 5634, 4438, 4439, 7547]\nTop 10 documents for Query 6483: [8362, 5634, 4438, 4439, 7547, 7448, 6168, 5635, 6482, 392]\n\nTop 5 documents for Query 3796: [3797, 3702, 2234, 2270, 2271]\nTop 10 documents for Query 3796: [3797, 3702, 2234, 2270, 2271, 3701, 8414, 8415, 2235, 7821]\n\nTop 5 documents for Query 5040: [9678, 8472, 5491, 4715, 5577]\nTop 10 documents for Query 5040: [9678, 8472, 5491, 4715, 5577, 2372, 1713, 1258, 3246, 2373]\n\nTop 5 documents for Query 6253: [7690, 2579, 6821, 9441, 4923]\nTop 10 documents for Query 6253: [7690, 2579, 6821, 9441, 4923, 9054, 414, 413, 2198, 2124]\n\nTop 5 documents for Query 3076: [3075, 486, 2048, 8859, 8818]\nTop 10 documents for Query 3076: [3075, 486, 2048, 8859, 8818, 7389, 7390, 9269, 485, 7260]\n\nTop 5 documents for Query 9965: [2441, 6837, 773, 4159, 9966]\nTop 10 documents for Query 9965: [2441, 6837, 773, 4159, 9966, 5539, 8830, 1014, 2712, 3038]\n\nTop 5 documents for Query 8456: [9585, 6026, 6060, 2098, 997]\nTop 10 documents for Query 8456: [9585, 6026, 6060, 2098, 997, 4373, 2235, 8489, 6352, 6353]\n\nTop 5 documents for Query 9615: [10053, 8236, 9614, 10054, 9335]\nTop 10 documents for Query 9615: [10053, 8236, 9614, 10054, 9335, 7907, 7752, 3073, 2637, 1153]\n\nTop 5 documents for Query 399: [2607, 2606, 9754, 400, 1779]\nTop 10 documents for Query 399: [2607, 2606, 9754, 400, 1779, 2120, 7753, 8355, 1846, 3691]\n\nTop 5 documents for Query 7372: [731, 7373, 2637, 3073, 3946]\nTop 10 documents for Query 7372: [731, 7373, 2637, 3073, 3946, 8358, 4451, 5000, 822, 8887]\n\nTop 5 documents for Query 1578: [10319, 3433, 3440, 3439, 3438]\nTop 10 documents for Query 1578: [10319, 3433, 3440, 3439, 3438, 3437, 3436, 3435, 3434, 3432]\n\nTop 5 documents for Query 8831: [1014, 8830, 5539, 6837, 2441]\nTop 10 documents for Query 8831: [1014, 8830, 5539, 6837, 2441, 773, 4159, 8400, 8120, 8914]\n\nTop 5 documents for Query 3459: [3460, 283, 6962, 4855, 2078]\nTop 10 documents for Query 3459: [3460, 283, 6962, 4855, 2078, 2079, 8309, 4820, 6167, 6766]\n\nTop 5 documents for Query 540: [7241, 541, 9108, 5738, 403]\nTop 10 documents for Query 540: [7241, 541, 9108, 5738, 403, 7242, 1199, 1406, 1407, 5611]\n\nTop 5 documents for Query 1956: [1955, 8054, 5155, 10233, 10232]\nTop 10 documents for Query 1956: [1955, 8054, 5155, 10233, 10232, 6019, 4592, 4591, 6020, 8628]\n\nTop 5 documents for Query 3017: [3018, 5437, 7710, 6859, 2389]\nTop 10 documents for Query 3017: [3018, 5437, 7710, 6859, 2389, 9438, 6468, 1682, 7075, 8776]\n\nTop 5 documents for Query 6209: [6210, 1383, 2538, 9516, 9631]\nTop 10 documents for Query 6209: [6210, 1383, 2538, 9516, 9631, 2499, 2324, 2325, 2660, 2659]\n\nTop 5 documents for Query 10155: [10154, 5521, 9051, 9604, 5625]\nTop 10 documents for Query 10155: [10154, 5521, 9051, 9604, 5625, 5522, 9887, 5349, 2007, 2008]\n\nTop 5 documents for Query 8055: [8054, 3541, 1955, 8628, 2815]\nTop 10 documents for Query 8055: [8054, 3541, 1955, 8628, 2815, 6453, 2587, 6956, 5440, 5439]\n\nTop 5 documents for Query 3322: [3321, 7303, 9474, 10109, 7448]\nTop 10 documents for Query 3322: [3321, 7303, 9474, 10109, 7448, 4438, 6168, 8362, 7547, 4439]\n\nTop 5 documents for Query 2658: [1923, 3118, 1314, 5346, 830]\nTop 10 documents for Query 2658: [1923, 3118, 1314, 5346, 830, 9124, 1313, 9408, 9112, 8347]\n\nTop 5 documents for Query 6936: [1043, 6954, 6935, 4158, 1320]\nTop 10 documents for Query 6936: [1043, 6954, 6935, 4158, 1320, 7605, 911, 1408, 1042, 2883]\n\nTop 5 documents for Query 548: [549, 319, 886, 6954, 6935]\nTop 10 documents for Query 548: [549, 319, 886, 6954, 6935, 4158, 1043, 7100, 4330, 4329]\n\nTop 5 documents for Query 6276: [6277, 4643, 2428, 6497, 4644]\nTop 10 documents for Query 6276: [6277, 4643, 2428, 6497, 4644, 8173, 1244, 6229, 5919, 6230]\n\nTop 5 documents for Query 609: [9930, 9698, 608, 9697, 2817]\nTop 10 documents for Query 609: [9930, 9698, 608, 9697, 2817, 2816, 5765, 1374, 713, 10306]\n\nTop 5 documents for Query 1350: [4182, 671, 5823, 10219, 8702]\nTop 10 documents for Query 1350: [4182, 671, 5823, 10219, 8702, 5824, 8703, 10218, 9136, 5212]\n\nTop 5 documents for Query 10188: [10189, 374, 2988, 2987, 5114]\nTop 10 documents for Query 10188: [10189, 374, 2988, 2987, 5114, 9380, 9379, 8837, 7764, 373]\n\nTop 5 documents for Query 2785: [2784, 859, 8165, 9594, 5550]\nTop 10 documents for Query 2785: [2784, 859, 8165, 9594, 5550, 407, 8166, 408, 6257, 1734]\n\nTop 5 documents for Query 3521: [8218, 7317, 4996, 4995, 3520]\nTop 10 documents for Query 3521: [8218, 7317, 4996, 4995, 3520, 9792, 2158, 4791, 4790, 1923]\n\nTop 5 documents for Query 9312: [9313, 664, 7052, 4277, 5639]\nTop 10 documents for Query 9312: [9313, 664, 7052, 4277, 5639, 1590, 8863, 5804, 1589, 921]\n\nTop 5 documents for Query 420: [419, 3310, 3309, 6074, 2495]\nTop 10 documents for Query 420: [419, 3310, 3309, 6074, 2495, 3482, 5084, 6073, 6706, 9032]\n\nTop 5 documents for Query 3959: [3960, 1785, 445, 446, 10233]\nTop 10 documents for Query 3959: [3960, 1785, 445, 446, 10233, 4591, 4592, 6019, 10232, 6020]\n\nTop 5 documents for Query 2822: [2802, 7989, 8030, 5518, 5166]\nTop 10 documents for Query 2822: [2802, 7989, 8030, 5518, 5166, 7993, 1297, 7856, 4044, 3795]\n\nTop 5 documents for Query 2145: [9394, 7241, 541, 2144, 7693]\nTop 10 documents for Query 2145: [9394, 7241, 541, 2144, 7693, 9708, 9108, 9311, 5738, 6850]\n\nTop 5 documents for Query 2678: [2677, 1647, 7558, 6573, 7559]\nTop 10 documents for Query 2678: [2677, 1647, 7558, 6573, 7559, 6609, 8494, 4136, 6891, 4872]\n\nTop 5 documents for Query 784: [6961, 6970, 1410, 8515, 7504]\nTop 10 documents for Query 784: [6961, 6970, 1410, 8515, 7504, 1411, 785, 1280, 1281, 3108]\n\nTop 5 documents for Query 5162: [5163, 2199, 9451, 106, 2200]\nTop 10 documents for Query 5162: [5163, 2199, 9451, 106, 2200, 4049, 1856, 554, 555, 3160]\n\nTop 5 documents for Query 7396: [7395, 5007, 8390, 240, 5008]\nTop 10 documents for Query 7396: [7395, 5007, 8390, 240, 5008, 1404, 7288, 1403, 8391, 5128]\n\nTop 5 documents for Query 1248: [2831, 926, 1249, 8341, 8340]\nTop 10 documents for Query 1248: [2831, 926, 1249, 8341, 8340, 8264, 927, 317, 7182, 4686]\n\nTop 5 documents for Query 7463: [881, 7464, 7073, 7074, 405]\nTop 10 documents for Query 7463: [881, 7464, 7073, 7074, 405, 5417, 5361, 5848, 812, 5360]\n\nTop 5 documents for Query 1453: [9409, 1452, 6120, 5757, 6119]\nTop 10 documents for Query 1453: [9409, 1452, 6120, 5757, 6119, 3558, 6613, 8040, 6772, 6614]\n\nTop 5 documents for Query 8458: [392, 391, 6482, 7547, 4438]\nTop 10 documents for Query 8458: [392, 391, 6482, 7547, 4438, 6168, 7448, 8362, 5634, 5635]\n\nTop 5 documents for Query 7222: [1790, 6546, 3651, 3277, 4398]\nTop 10 documents for Query 7222: [1790, 6546, 3651, 3277, 4398, 4397, 3278, 1540, 6223, 3212]\n\nTop 5 documents for Query 5516: [5515, 8742, 7015, 7014, 10194]\nTop 10 documents for Query 5516: [5515, 8742, 7015, 7014, 10194, 10195, 3338, 1648, 4785, 4784]\n\nTop 5 documents for Query 1992: [1991, 8655, 5083, 9079, 7484]\nTop 10 documents for Query 1992: [1991, 8655, 5083, 9079, 7484, 7066, 10064, 6077, 6635, 6078]\n\nTop 5 documents for Query 9376: [9375, 4335, 606, 8981, 4199]\nTop 10 documents for Query 9376: [9375, 4335, 606, 8981, 4199, 2577, 8127, 8126, 4336, 10208]\n\nTop 5 documents for Query 9214: [9213, 10116, 3625, 5774, 2014]\nTop 10 documents for Query 9214: [9213, 10116, 3625, 5774, 2014, 430, 10115, 9855, 5978, 2643]\n\nTop 5 documents for Query 11591: [8101, 8102, 1694, 9036, 2558]\nTop 10 documents for Query 11591: [8101, 8102, 1694, 9036, 2558, 7016, 3595, 1693, 2984, 3794]\n\nTop 5 documents for Query 4356: [1447, 4355, 6914, 353, 3839]\nTop 10 documents for Query 4356: [1447, 4355, 6914, 353, 3839, 739, 2722, 6683, 354, 6401]\n\nTop 5 documents for Query 5851: [5019, 1476, 5852, 10055, 5262]\nTop 10 documents for Query 5851: [5019, 1476, 5852, 10055, 5262, 5020, 9675, 4129, 2932, 4253]\n\nTop 5 documents for Query 2661: [2662, 8357, 9252, 3078, 9894]\nTop 10 documents for Query 2661: [2662, 8357, 9252, 3078, 9894, 4783, 6701, 10143, 10144, 2836]\n\nTop 5 documents for Query 6957: [8362, 5634, 4438, 4439, 7547]\nTop 10 documents for Query 6957: [8362, 5634, 4438, 4439, 7547, 7448, 6168, 5635, 6482, 392]\n\nTop 5 documents for Query 12584: [248, 247, 9512, 3315, 8094]\nTop 10 documents for Query 12584: [248, 247, 9512, 3315, 8094, 6426, 2582, 9511, 6161, 2111]\n\nTop 5 documents for Query 8306: [8618, 9879, 8619, 6929, 8307]\nTop 10 documents for Query 8306: [8618, 9879, 8619, 6929, 8307, 2371, 2370, 5511, 4890, 8037]\n\nTop 5 documents for Query 6244: [7580, 1659, 1660, 6243, 1813]\nTop 10 documents for Query 6244: [7580, 1659, 1660, 6243, 1813, 1814, 8312, 8505, 3181, 5756]\n\nTop 5 documents for Query 8482: [8481, 6691, 6692, 7454, 8073]\nTop 10 documents for Query 8482: [8481, 6691, 6692, 7454, 8073, 299, 4377, 890, 891, 1474]\n\nTop 5 documents for Query 858: [2784, 859, 6258, 8879, 8165]\nTop 10 documents for Query 858: [2784, 859, 6258, 8879, 8165, 9594, 5550, 407, 1327, 1328]\n\nTop 5 documents for Query 7105: [2767, 9913, 805, 9914, 884]\nTop 10 documents for Query 7105: [2767, 9913, 805, 9914, 884, 2411, 3434, 3441, 3440, 3439]\n\nTop 5 documents for Query 1166: [1167, 6162, 2951, 1554, 5540]\nTop 10 documents for Query 1166: [1167, 6162, 2951, 1554, 5540, 1553, 7151, 4236, 1030, 5801]\n\nTop 5 documents for Query 9340: [6303, 10083, 6302, 1381, 1392]\nTop 10 documents for Query 9340: [6303, 10083, 6302, 1381, 1392, 260, 259, 8099, 2003, 2231]\n\nTop 5 documents for Query 975: [974, 3370, 772, 6301, 4365]\nTop 10 documents for Query 975: [974, 3370, 772, 6301, 4365, 3446, 8411, 7194, 8467, 3038]\n\nTop 5 documents for Query 379: [380, 1799, 1800, 912, 1103]\nTop 10 documents for Query 379: [380, 1799, 1800, 912, 1103, 1020, 2216, 1102, 5018, 10300]\n\nTop 5 documents for Query 1164: [2683, 5369, 2684, 1165, 2182]\nTop 10 documents for Query 1164: [2683, 5369, 2684, 1165, 2182, 469, 7021, 2855, 4099, 2120]\n\nTop 5 documents for Query 12508: [2995, 8207, 5096, 2114, 9191]\nTop 10 documents for Query 12508: [2995, 8207, 5096, 2114, 9191, 4060, 8283, 2115, 9192, 1988]\n\nTop 5 documents for Query 1079: [1078, 5653, 1404, 1403, 1082]\nTop 10 documents for Query 1079: [1078, 5653, 1404, 1403, 1082, 7923, 9896, 2694, 1243, 9895]\n","output_type":"stream"}]},{"cell_type":"code","source":"# Dict of relevent documents for evalution purpose\nrelevant_documents_dict =  query_eval.set_index('query_id')['doc_id'].to_dict()\n\n# Calculate Precision@k for each query\ndef precision_at_k(query_id, top_k):\n    relevant_doc = relevant_documents_dict.get(query_id, None)\n    if relevant_doc is not None:\n        return int(relevant_doc in top_k)\n    return 0\n\n# Calculate Precision@1, Precision@5, and Precision@10 for each query\nprecision_at_1 = [precision_at_k(quaries.loc[query_id]['query_id'], z[:1]) for query_id, z in enumerate(top5)]\nprecision_at_5 = [precision_at_k(quaries.loc[query_id]['query_id'], x) for query_id, x in enumerate(top5)]\nprecision_at_10 = [precision_at_k(quaries.loc[query_id]['query_id'], y) for query_id, y in enumerate(top10)]\n# Average Precision@k over all queries\navg_precision_at_1 = sum(precision_at_1) / len(precision_at_1)\navg_precision_at_5 = sum(precision_at_5) / len(precision_at_5)\navg_precision_at_10 = sum(precision_at_10) / len(precision_at_10)\n\n# Report Precision@k scores\nprint(f\"Avg Precision@1: {avg_precision_at_1}\")\nprint(f\"Avg Precision@5: {avg_precision_at_5}\")\nprint(f\"Avg Precision@10: {avg_precision_at_10}\")","metadata":{"execution":{"iopub.status.busy":"2024-02-04T15:06:29.511640Z","iopub.execute_input":"2024-02-04T15:06:29.512051Z","iopub.status.idle":"2024-02-04T15:06:29.550790Z","shell.execute_reply.started":"2024-02-04T15:06:29.512018Z","shell.execute_reply":"2024-02-04T15:06:29.549358Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"Avg Precision@1: 0.48\nAvg Precision@5: 0.74\nAvg Precision@10: 0.77\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Task 2","metadata":{}},{"cell_type":"markdown","source":"*  Improve the performance of Task1 by stemming the tokens (using spacy) before calculating the vocabulary.\n*  Improve the performance of Task1 by lemmatizing the tokens (using spacy) before calculating the vocabulary.\n*  Report the size of the vocabulary you obtained as part of Task 1, the vocabulary size after stemming and the vocabulary size after lemmatization.\n*  Report the performance metrics in both these cases and discuss the results (why or why not performance has increased).\n","metadata":{}},{"cell_type":"code","source":"# By stemming the tokens\nprint(\"The vocabulary size before stemming: \",vectorizer.get_feature_names_out().shape)\n\nstemmer = PorterStemmer()\ntoken = spacy.load(\"en_core_web_sm\")\ntokenized_docs = []\nall_tokens = []\n\nfor i, row in document.iterrows():\n    doc_text = row['doc_text']\n    doc_tokens = [stemmer.stem(tok.text.lower()) for tok in token(doc_text) if not tok.is_stop and tok.is_alpha]\n    tokenized_docs.append(doc_tokens)\n    all_tokens.extend(doc_tokens)\n\n#countinf the frequency of tokens\nword_freq = Counter(all_tokens)\n# Filter out tokens\nmin_freq = 5\ndoc_freq = 0.85\n\nfiltered_tokens=[]\nfor token, count in word_freq.items():\n    if min_freq <= count <= len(document) * doc_freq:\n        filtered_tokens.append(token)\n\n# Tokenize documents and queries using the filtered vocabulary\ntokenized_docs = []\ntoken = spacy.load(\"en_core_web_sm\")\nfor doc_text in document['doc_text']:\n    doc_tokens = [stemmer.stem(tok.text.lower()) for tok in token(doc_text) if stemmer.stem(tok.lemma_) in filtered_tokens]\n    tokenized_docs.append(doc_tokens)\n\ntokenized_queries = []\n\nfor query_text in quaries['query_text']:\n    query_tokens = [stemmer.stem(tok.text.lower()) for tok in token(query_text) if stemmer.stem(tok.lemma_) in filtered_tokens]\n    tokenized_queries.append(query_tokens)\n\n    \n#print(tokenized_docs[0])\n# print(tokenized_queries)\nvectorizer1 = TfidfVectorizer()\nmatrix = vectorizer1.fit_transform([\" \".join(tokens) for tokens in tokenized_docs + tokenized_queries])\ndoc_matrix = matrix[:len(document)]\nqueries_matrix = matrix[len(document):]\nprint(\"The vocabulary size After Stemming : \",vectorizer1.get_feature_names_out().shape)","metadata":{"execution":{"iopub.status.busy":"2024-02-04T15:11:03.873429Z","iopub.execute_input":"2024-02-04T15:11:03.873976Z","iopub.status.idle":"2024-02-04T15:14:16.185342Z","shell.execute_reply.started":"2024-02-04T15:11:03.873939Z","shell.execute_reply":"2024-02-04T15:14:16.184001Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"The vocabulary size before stemming:  (2129,)\nThe vocabulary size After Stemming :  (2026,)\n","output_type":"stream"}]},{"cell_type":"code","source":"# Performance After Stemming and lemmitizations\n# 2.4 find the cosine similarity of each query\ncosine_similarities = cosine_similarity(queries_matrix, doc_matrix)\n# for each query find the top 5 and top 10 queries from document\ni=0\ntop5=[]\ntop10=[]\nfor similarities in cosine_similarities:\n    top5.append([document.loc[i]['doc_id'] for i in np.argsort(similarities)[::-1][:5]])\n    top10.append([document.loc[i]['doc_id'] for i in np.argsort(similarities)[::-1][:10]])\n    #print(f\"\\nTop 5 documents for Query {quaries.loc[i]['query_id']}: {top5[-1]}\")\n    #print(f\"Top 10 documents for Query {quaries.loc[i]['query_id']}: {top10[-1]}\")\n    i+=1  \n# Calculate Precision@1, Precision@5, and Precision@10 for each query\nprecision_at_1 = [precision_at_k(quaries.loc[query_id]['query_id'], z[:1]) for query_id, z in enumerate(top5)]\nprecision_at_5 = [precision_at_k(quaries.loc[query_id]['query_id'], x) for query_id, x in enumerate(top5)]\nprecision_at_10 = [precision_at_k(quaries.loc[query_id]['query_id'], y) for query_id, y in enumerate(top10)]\n# Average Precision@k over all queries\navg_precision_at_1 = sum(precision_at_1) / len(precision_at_1)\navg_precision_at_5 = sum(precision_at_5) / len(precision_at_5)\navg_precision_at_10 = sum(precision_at_10) / len(precision_at_10)\n\n# Report Precision@k scores\nprint(f\"Avg Precision@1: {avg_precision_at_1}\")\nprint(f\"Avg Precision@5: {avg_precision_at_5}\")\nprint(f\"Avg Precision@10: {avg_precision_at_10}\")","metadata":{"execution":{"iopub.status.busy":"2024-02-04T15:14:51.196558Z","iopub.execute_input":"2024-02-04T15:14:51.197780Z","iopub.status.idle":"2024-02-04T15:14:51.390099Z","shell.execute_reply.started":"2024-02-04T15:14:51.197730Z","shell.execute_reply":"2024-02-04T15:14:51.388604Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"Avg Precision@1: 0.57\nAvg Precision@5: 0.81\nAvg Precision@10: 0.83\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### using lemmatization","metadata":{}},{"cell_type":"code","source":"# By stemming the tokens\nprint(\"The vocabulary size before Lemmatization: \",vectorizer.get_feature_names_out().shape)\n\nstemmer = PorterStemmer()\ntoken = spacy.load(\"en_core_web_sm\")\ntokenized_docs = []\nall_tokens = []\n\nfor i, row in document.iterrows():\n    doc_text = row['doc_text']\n    doc_tokens = [tok.lemma_ for tok in token(doc_text) if not tok.is_stop and tok.is_alpha]\n    tokenized_docs.append(doc_tokens)\n    all_tokens.extend(doc_tokens)\n\n#countinf the frequency of tokens\nword_freq = Counter(all_tokens)\n# Filter out tokens\nmin_freq = 5\ndoc_freq = 0.85\n\nfiltered_tokens=[]\nfor token, count in word_freq.items():\n    if min_freq <= count <= len(document) * doc_freq:\n        filtered_tokens.append(token)\n\n# Tokenize documents and queries using the filtered vocabulary\ntokenized_docs = []\ntoken = spacy.load(\"en_core_web_sm\")\nfor doc_text in document['doc_text']:\n    doc_tokens = [tok.lemma_ for tok in token(doc_text) if stemmer.stem(tok.lemma_) in filtered_tokens]\n    tokenized_docs.append(doc_tokens)\n\ntokenized_queries = []\n\nfor query_text in quaries['query_text']:\n    query_tokens = [tok.lemma_ for tok in token(query_text) if stemmer.stem(tok.lemma_) in filtered_tokens]\n    tokenized_queries.append(query_tokens)\n\nvectorizer1 = TfidfVectorizer()\nmatrix = vectorizer1.fit_transform([\" \".join(tokens) for tokens in tokenized_docs + tokenized_queries])\ndoc_matrix = matrix[:len(document)]\nqueries_matrix = matrix[len(document):]\n\n# get the vocabulary size\nprint(\"The vocabulary size After Lemmatization : \",vectorizer1.get_feature_names_out().shape)","metadata":{"execution":{"iopub.status.busy":"2024-02-04T15:30:43.756371Z","iopub.execute_input":"2024-02-04T15:30:43.758230Z","iopub.status.idle":"2024-02-04T15:33:49.740679Z","shell.execute_reply.started":"2024-02-04T15:30:43.758166Z","shell.execute_reply":"2024-02-04T15:33:49.739417Z"},"trusted":true},"execution_count":26,"outputs":[{"name":"stdout","text":"The vocabulary size before Lemmatization:  (2129,)\nThe vocabulary size After Lemmatization :  (1380,)\n","output_type":"stream"}]},{"cell_type":"code","source":"# Performance After Stemming and lemmitizations\n# 2.4 find the cosine similarity of each query\ncosine_similarities = cosine_similarity(queries_matrix, doc_matrix)\n# for each query find the top 5 and top 10 queries from document\ni=0\ntop5=[]\ntop10=[]\nfor similarities in cosine_similarities:\n    top5.append([document.loc[i]['doc_id'] for i in np.argsort(similarities)[::-1][:5]])\n    top10.append([document.loc[i]['doc_id'] for i in np.argsort(similarities)[::-1][:10]])\n    #print(f\"\\nTop 5 documents for Query {quaries.loc[i]['query_id']}: {top5[-1]}\")\n    #print(f\"Top 10 documents for Query {quaries.loc[i]['query_id']}: {top10[-1]}\")\n    i+=1  \n# Calculate Precision@1, Precision@5, and Precision@10 for each query\nprecision_at_1 = [precision_at_k(quaries.loc[query_id]['query_id'], z[:1]) for query_id, z in enumerate(top5)]\nprecision_at_5 = [precision_at_k(quaries.loc[query_id]['query_id'], x) for query_id, x in enumerate(top5)]\nprecision_at_10 = [precision_at_k(quaries.loc[query_id]['query_id'], y) for query_id, y in enumerate(top10)]\n# Average Precision@k over all queries\navg_precision_at_1 = sum(precision_at_1) / len(precision_at_1)\navg_precision_at_5 = sum(precision_at_5) / len(precision_at_5)\navg_precision_at_10 = sum(precision_at_10) / len(precision_at_10)\n\n# Report Precision@k scores\nprint(f\"Avg Precision@1: {avg_precision_at_1}\")\nprint(f\"Avg Precision@5: {avg_precision_at_5}\")\nprint(f\"Avg Precision@10: {avg_precision_at_10}\")","metadata":{"execution":{"iopub.status.busy":"2024-02-04T15:20:26.320461Z","iopub.execute_input":"2024-02-04T15:20:26.320982Z","iopub.status.idle":"2024-02-04T15:20:26.510412Z","shell.execute_reply.started":"2024-02-04T15:20:26.320947Z","shell.execute_reply":"2024-02-04T15:20:26.509243Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"Avg Precision@1: 0.34\nAvg Precision@5: 0.56\nAvg Precision@10: 0.64\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Task 3","metadata":{}},{"cell_type":"markdown","source":"*   Improve the model from Task 2.2 further with Named Entity Recognition (NER) and Parts-Of-Speech (POS) tagging using spaCy.\n*   For each query and document vector, give more weightage to some important words. In essence, for each of the tf-idf vectors, multiply 2 along the dimensions which contain nouns, and multiply 4 for the named entities.\n*   Report the performance metrics ","metadata":{}},{"cell_type":"code","source":"print(\"Task3: \")\n# extract_E_P is a function to extract named entities and POS tags\ntoken = spacy.load(\"en_core_web_sm\")\nspecial_case = [{\"ORTH\": \"id\", \"NORM\": \"id\"}]\nspecial_case_wed = [{\"ORTH\": \"wed\", \"NORM\": \"wed\"}]\ntoken.tokenizer.add_special_case(\"id\", special_case)\ntoken.tokenizer.add_special_case(\"wed\", special_case_wed)\ndef entity_tags(text):\n    doc = token(text)\n    entity = [ent.text for ent in doc.ents]\n    tags = [tok.pos_ for tok in doc]\n    return entity, tags\n\n# find tags and enties for both documents and quaries\ndocument_entities, document_tags = zip(*document['doc_text'].apply(entity_tags))\nquaries_entities, quaries_tags = zip(*quaries['query_text'].apply(entity_tags))\n\n# Append Named Entity Recognition (NER) and Parts-Of-Speech features to TF-IDF matrix\nk=vectorizer1.get_feature_names_out()\nprint(\"\\n The vocabulary size : \",k.shape)\nx,y=entity_tags(' '.join(k))\n\n# modify the tf_idf vector as per given in assignment\ndef modify(tfidf_vector, pos_tags, entities):\n    noun_indices = [i for i, pos_tag in enumerate(pos_tags) if 'NOUN' in pos_tag]\n    tfidf_vector[:, noun_indices] *= 2 \n    entity_indices = [i for i,j in enumerate(k) if j in entities]\n    tfidf_vector[:, entity_indices] *= 4\n\n    return tfidf_vector\n#do the modifications in doc and queries matrix\ndoc_matrix_modify = modify(doc_matrix, y,x)\nqueries_matrix_modify = modify(queries_matrix, y, x)\n\n# Calculating cosine similarity \ncosine_similarities = cosine_similarity(queries_matrix_modify, doc_matrix_modify)\n#print(cosine_similarities)\n# for each query find the top 5 and top 10 queries from document\ni=0\ntop5=[]\ntop10=[]\nfor similarities in cosine_similarities:\n    top5.append([document.loc[i]['doc_id'] for i in np.argsort(similarities)[::-1][:5]])\n    top10.append([document.loc[i]['doc_id'] for i in np.argsort(similarities)[::-1][:10]])\n    #print(f\"\\nTop 5 documents for Query {quaries.loc[i]['query_id']}: {top5[-1]}\")\n    #print(f\"Top 10 documents for Query {quaries.loc[i]['query_id']}: {top10[-1]}\")\n    i+=1  \n# Calculate Precision@1, Precision@5, and Precision@10 for each query\nprecision_at_1 = [precision_at_k(quaries.loc[query_id]['query_id'], a[:1]) for query_id, a in enumerate(top5)]\nprecision_at_5 = [precision_at_k(quaries.loc[query_id]['query_id'], b) for query_id, b in enumerate(top5)]\nprecision_at_10 = [precision_at_k(quaries.loc[query_id]['query_id'], c) for query_id, c in enumerate(top10)]\n# Average Precision@k over all queries\navg_precision_at_1 = sum(precision_at_1) / len(precision_at_1)\navg_precision_at_5 = sum(precision_at_5) / len(precision_at_5)\navg_precision_at_10 = sum(precision_at_10) / len(precision_at_10)\n\n# Report Precision@k scores\nprint(f\"Avg Precision@1: {avg_precision_at_1}\")\nprint(f\"Avg Precision@5: {avg_precision_at_5}\")\nprint(f\"Avg Precision@10: {avg_precision_at_10}\")","metadata":{"execution":{"iopub.status.busy":"2024-02-04T15:28:37.446219Z","iopub.execute_input":"2024-02-04T15:28:37.446744Z","iopub.status.idle":"2024-02-04T15:30:07.859960Z","shell.execute_reply.started":"2024-02-04T15:28:37.446708Z","shell.execute_reply":"2024-02-04T15:30:07.858466Z"},"trusted":true},"execution_count":25,"outputs":[{"name":"stdout","text":"Task3: \n\n The vocabulary size :  (1380,)\nAvg Precision@1: 0.35\nAvg Precision@5: 0.55\nAvg Precision@10: 0.59\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Task 4:\n\n- Performing the both lemmatization and stemming both to improve the performace ","metadata":{}},{"cell_type":"code","source":"# By stemming the tokens\nprint(\"The vocabulary size before stemming along with Lemmatization: \",vectorizer.get_feature_names_out().shape)\n\nstemmer = PorterStemmer()\ntoken = spacy.load(\"en_core_web_sm\")\ntokenized_docs = []\nall_tokens = []\n\nfor i, row in document.iterrows():\n    doc_text = row['doc_text']\n    doc_tokens = [stemmer.stem(tok.lemma_) for tok in token(doc_text) if not tok.is_stop and tok.is_alpha]\n    tokenized_docs.append(doc_tokens)\n    all_tokens.extend(doc_tokens)\n\n#countinf the frequency of tokens\nword_freq = Counter(all_tokens)\n# Filter out tokens\nmin_freq = 5\ndoc_freq = 0.85\n\nfiltered_tokens=[]\nfor token, count in word_freq.items():\n    if min_freq <= count <= len(document) * doc_freq:\n        filtered_tokens.append(token)\n\n# Tokenize documents and queries using the filtered vocabulary\ntokenized_docs = []\ntoken = spacy.load(\"en_core_web_sm\")\nfor doc_text in document['doc_text']:\n    doc_tokens = [stemmer.stem(tok.lemma_) for tok in token(doc_text) if stemmer.stem(tok.lemma_) in filtered_tokens]\n    tokenized_docs.append(doc_tokens)\n\ntokenized_queries = []\n\nfor query_text in quaries['query_text']:\n    query_tokens = [stemmer.stem(tok.lemma_) for tok in token(query_text) if stemmer.stem(tok.lemma_) in filtered_tokens]\n    tokenized_queries.append(query_tokens)\n\nvectorizer1 = TfidfVectorizer()\nmatrix = vectorizer1.fit_transform([\" \".join(tokens) for tokens in tokenized_docs + tokenized_queries])\ndoc_matrix = matrix[:len(document)]\nqueries_matrix = matrix[len(document):]\n# get the vocabulary size\nprint(\"The vocabulary size After Stemming along with Lemmatization : \",vectorizer1.get_feature_names_out().shape)","metadata":{"execution":{"iopub.status.busy":"2024-02-04T15:34:19.831994Z","iopub.execute_input":"2024-02-04T15:34:19.832547Z","iopub.status.idle":"2024-02-04T15:37:28.937008Z","shell.execute_reply.started":"2024-02-04T15:34:19.832506Z","shell.execute_reply":"2024-02-04T15:37:28.935550Z"},"trusted":true},"execution_count":27,"outputs":[{"name":"stdout","text":"\n Improve the performance of Task1 by stemming the tokens \n\nThe vocabulary size before stemming:  (2129,)\nThe vocabulary size After Stemming :  (1912,)\n","output_type":"stream"}]},{"cell_type":"code","source":"# Performance After Stemming and lemmitizations\n# 2.4 find the cosine similarity of each query\ncosine_similarities = cosine_similarity(queries_matrix, doc_matrix)\n# for each query find the top 5 and top 10 queries from document\ni=0\ntop5=[]\ntop10=[]\nfor similarities in cosine_similarities:\n    top5.append([document.loc[i]['doc_id'] for i in np.argsort(similarities)[::-1][:5]])\n    top10.append([document.loc[i]['doc_id'] for i in np.argsort(similarities)[::-1][:10]])\n    #print(f\"\\nTop 5 documents for Query {quaries.loc[i]['query_id']}: {top5[-1]}\")\n    #print(f\"Top 10 documents for Query {quaries.loc[i]['query_id']}: {top10[-1]}\")\n    i+=1  \n# Calculate Precision@1, Precision@5, and Precision@10 for each query\nprecision_at_1 = [precision_at_k(quaries.loc[query_id]['query_id'], z[:1]) for query_id, z in enumerate(top5)]\nprecision_at_5 = [precision_at_k(quaries.loc[query_id]['query_id'], x) for query_id, x in enumerate(top5)]\nprecision_at_10 = [precision_at_k(quaries.loc[query_id]['query_id'], y) for query_id, y in enumerate(top10)]\n# Average Precision@k over all queries\navg_precision_at_1 = sum(precision_at_1) / len(precision_at_1)\navg_precision_at_5 = sum(precision_at_5) / len(precision_at_5)\navg_precision_at_10 = sum(precision_at_10) / len(precision_at_10)\n\n# Report Precision@k scores\nprint(f\"Avg Precision@1: {avg_precision_at_1}\")\nprint(f\"Avg Precision@5: {avg_precision_at_5}\")\nprint(f\"Avg Precision@10: {avg_precision_at_10}\")","metadata":{"execution":{"iopub.status.busy":"2024-02-04T15:37:28.938896Z","iopub.execute_input":"2024-02-04T15:37:28.939222Z","iopub.status.idle":"2024-02-04T15:37:29.123450Z","shell.execute_reply.started":"2024-02-04T15:37:28.939195Z","shell.execute_reply":"2024-02-04T15:37:29.122291Z"},"trusted":true},"execution_count":28,"outputs":[{"name":"stdout","text":"Avg Precision@1: 0.57\nAvg Precision@5: 0.82\nAvg Precision@10: 0.83\n","output_type":"stream"}]}]}